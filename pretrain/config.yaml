data_dir: "{__dir__}/../_data"
exp_label: "_pretrain"

dataset:
  wiki:
    hf_config: 20220301.en
    split_sizes_MB: {train: 1024, test: 256}
    shuffle: true
    random_seed: 0
    sent_min_spaces: 5
    sent_max_spaces: 200
    text_col: text
    output_subdir: "wiki/20220301.en.1gb"
    save_compressed: true

train:
  dataset: wiki
  vocab_size: 20_000
  base_model: bert-base-cased
  max_length: 128
  tokenizer_subdir: "tokenizer"
  model_subdir: "model"

  tokenize_params: {}

  mlm_probability: 0.15

  bert_config: {}

  training_args:
    num_train_epochs: 1
    per_device_train_batch_size: 128
    save_steps: 10_000
    save_total_limit: 2
    prediction_loss_only: true
