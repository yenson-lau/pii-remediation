data_directory: "{__dir__}/../_data"

pretrain:
  subdirectory: &model_subdir models/bert
  tokenizer: wordpiece

  bert_config: {}
  mlm_probability: 0.15
  training_args:
    output_dir: *model_subdir
    overwrite_output_dir: true
    train_num_epochs: 3,
    save_steps: 10_000
    save_total_limit: 2
    prediction_loss_only: true

tokenizer:
  subdirectory: tokenizer

  wordpiece:
    subdirectory: wordpiece
    vocab_size: 20_000
    strip_accents: true
    lowercase: false
    special_tokens: ["[UNK]", "[SEP]", "[PAD]", "[CLS]", "[MASK]"]
    batch_size: 1000

ner: null
