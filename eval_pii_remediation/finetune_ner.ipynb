{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import pandas as pd\n",
    "from pprint import PrettyPrinter, pprint\n",
    "from typing import Optional\n",
    "\n",
    "__DIR__ = globals()['_dh'][0]\n",
    "data_dir = path.relpath(path.join(__DIR__, \"..\", \"_data\"))\n",
    "\n",
    "pp = PrettyPrinter(indent=2, width=120)\n",
    "\n",
    "pd.set_option('display.width', 120)\n",
    "pd.set_option('display.max_colwidth', 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "_colab_install = True\n",
    "_testing = True\n",
    "\n",
    "# Parameters\n",
    "tokenizer_dir = path.join(data_dir, \"pretrain\", \"tokenizer\")\n",
    "model_dir = path.join(data_dir, \"pretrain\", \"model\")\n",
    "ner_dir = path.join(data_dir, \"ner\")\n",
    "\n",
    "null_label = -100   # https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a\n",
    "max_length = 128\n",
    "\n",
    "training_args = dict(\n",
    "    optim = \"adamw_torch\",\n",
    "    num_train_epochs = 5,\n",
    "    per_device_train_batch_size = 64,\n",
    "    eval_accumulation_steps = 10,\n",
    "\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    logging_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    \n",
    "    load_best_model_at_end = True\n",
    "    save_total_limit = 3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process settings / parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _testing:\n",
    "    training_args.update(dict(\n",
    "        num_train_epochs = 1\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER finetuning on conllpp dataset:\n",
      "OrderedDict([ ('tokenizer_dir', '../_data/pretrain/tokenizer'),\n",
      "              ('model_dir', '../_data/pretrain/model'),\n",
      "              ('ner_dir', '../_data/ner'),\n",
      "              ('null_label', -100),\n",
      "              ('max_length', 128),\n",
      "              ( 'training_args',\n",
      "                { 'eval_accumulation_steps': 10,\n",
      "                  'evaluation_strategy': 'epoch',\n",
      "                  'logging_strategy': 'epoch',\n",
      "                  'num_train_epochs': 1,\n",
      "                  'optim': 'adamw_torch',\n",
      "                  'per_device_train_batch_size': 64,\n",
      "                  'save_total_limit': 3})])\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "if _colab_install:\n",
    "    try:\n",
    "        import google.colab\n",
    "        \n",
    "        colab_install_script = path.join(__DIR__, \"..\", \"colab_install.sh\")\n",
    "\n",
    "        if not path.isfile(colab_install_script):\n",
    "            script_url = \"https://raw.githubusercontent.com/yenson-lau/pii-remediation/main/colab_install.sh\"\n",
    "            !wget $script_url -O $colab_install_script\n",
    "\n",
    "        !bash $colab_install_script\n",
    "        print()\n",
    "\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "\n",
    "config = OrderedDict(\n",
    "    tokenizer_dir = tokenizer_dir,\n",
    "    model_dir = model_dir,\n",
    "    ner_dir = ner_dir,\n",
    "\n",
    "    null_label = null_label,\n",
    "    max_length = max_length,\n",
    "\n",
    "    training_args = training_args,\n",
    ")\n",
    "\n",
    "print(\"NER finetuning on conllpp dataset:\")\n",
    "pp.pprint(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load / process dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conllpp (/Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01b0b43694ab4881b22cad505759cf64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "      <td>[22, 42, 16, 21, 35, 37, 16, 21, 7]</td>\n",
       "      <td>[11, 21, 11, 12, 21, 22, 11, 12, 0]</td>\n",
       "      <td>[3, 0, 7, 0, 0, 0, 7, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>[22, 22]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[1, 2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>[22, 11]</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>[5, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, t...</td>\n",
       "      <td>[12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 1...</td>\n",
       "      <td>[11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 1...</td>\n",
       "      <td>[0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Wer...</td>\n",
       "      <td>[22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 1...</td>\n",
       "      <td>[11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1...</td>\n",
       "      <td>[5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                                                                     tokens  \\\n",
       "0  0                                 [EU, rejects, German, call, to, boycott, British, lamb, .]   \n",
       "1  1                                                                         [Peter, Blackburn]   \n",
       "2  2                                                                     [BRUSSELS, 1996-08-22]   \n",
       "3  3  [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, t...   \n",
       "4  4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Wer...   \n",
       "\n",
       "                                                                                    pos_tags  \\\n",
       "0                                                        [22, 42, 16, 21, 35, 37, 16, 21, 7]   \n",
       "1                                                                                   [22, 22]   \n",
       "2                                                                                   [22, 11]   \n",
       "3  [12, 22, 22, 38, 15, 22, 28, 38, 15, 16, 21, 35, 24, 35, 37, 16, 21, 15, 24, 41, 15, 1...   \n",
       "4  [22, 27, 21, 35, 12, 22, 22, 27, 16, 21, 22, 22, 38, 15, 22, 24, 20, 37, 21, 15, 24, 1...   \n",
       "\n",
       "                                                                                  chunk_tags  \\\n",
       "0                                                        [11, 21, 11, 12, 21, 22, 11, 12, 0]   \n",
       "1                                                                                   [11, 12]   \n",
       "2                                                                                   [11, 12]   \n",
       "3  [11, 12, 12, 21, 13, 11, 11, 21, 13, 11, 12, 13, 11, 21, 22, 11, 12, 17, 11, 21, 17, 1...   \n",
       "4  [11, 11, 12, 13, 11, 12, 12, 11, 12, 12, 12, 12, 21, 13, 11, 12, 21, 22, 11, 13, 11, 1...   \n",
       "\n",
       "                                                                                    ner_tags  \n",
       "0                                                                [3, 0, 7, 0, 0, 0, 7, 0, 0]  \n",
       "1                                                                                     [1, 2]  \n",
       "2                                                                                     [5, 0]  \n",
       "3  [0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "4  [5, 0, 0, 0, 0, 3, 4, 0, 0, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"conllpp\")\n",
    "num_classes = dataset[\"train\"].features[\"ner_tags\"].feature.num_classes\n",
    "\n",
    "display(pd.DataFrame(dataset[\"train\"][:5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-074e680a38e10812.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-3dcb986fe396d27d.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-7d54e14e0c35d295.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-a32997c640cc8418.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bf1d9453b72b05ba.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-847866618d3c1cb2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-1b5305b561029d08.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-a2be676345c8ca8a.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-805d7a9ad640102e.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-02562645a2bb1a93.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-3790734082dac5d9.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/yenson/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-86ab6f968f43387c.arrow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>words</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2, 11829, 6647, 99, 885, 907, 179, 5989, 15573, 1228, 754, 113, 103, 18, 3, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[None, 0, 1, 1, 2, 3, 4, 5, 5, 6, 7, 7, 7, 8, None, None, None, None, None, None, None...</td>\n",
       "      <td>[-100, 3, 0, -100, 7, 0, 0, 0, -100, 7, 0, -100, -100, 0, -100, -100, -100, -100, -100...</td>\n",
       "      <td>[EU, rejects, German, call, to, boycott, British, lamb, .]</td>\n",
       "      <td>EU rejects German call to boycott British lamb.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 3501, 2503, 3534, 110, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[None, 0, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, N...</td>\n",
       "      <td>[-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -...</td>\n",
       "      <td>[Peter, Blackburn]</td>\n",
       "      <td>Peter Blackburn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, 37, 1761, 107, 3536, 143, 107, 1801, 17, 8785, 17, 1908, 3, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None...</td>\n",
       "      <td>[-100, 5, -100, -100, -100, -100, -100, 0, -100, -100, -100, -100, -100, -100, -100, -...</td>\n",
       "      <td>[BRUSSELS, 1996-08-22]</td>\n",
       "      <td>BRUSSELS 1996 - 08 - 22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2, 192, 2801, 5047, 1411, 201, 389, 1525, 1850, 314, 480, 16107, 159, 219, 885, 1831,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 7, 7, 8, 9, 10, 10, 11, 12, 13, 14, 14, 15, 16, 1...</td>\n",
       "      <td>[-100, 0, 3, 4, 0, 0, 0, -100, -100, 0, 0, -100, -100, 0, 7, 0, -100, 0, 0, 0, 0, -100...</td>\n",
       "      <td>[The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, t...</td>\n",
       "      <td>The European Commission said on Thursday it disagreed with German advice to consumers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2, 1963, 11, 83, 7565, 179, 155, 2801, 2096, 11, 83, 10315, 15768, 8244, 2157, 10340,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "      <td>[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 8, 9, 10, 10, 10, 11, 11, 11, 11, 12, 13, 14, ...</td>\n",
       "      <td>[-100, 5, 0, -100, 0, 0, 0, 3, 4, 0, -100, 0, -100, 0, 1, -100, -100, 2, -100, -100, -...</td>\n",
       "      <td>[Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Wer...</td>\n",
       "      <td>Germany's representative to the European Union's veterinary committee Werner Zwingmann...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   input_ids  \\\n",
       "0  [2, 11829, 6647, 99, 885, 907, 179, 5989, 15573, 1228, 754, 113, 103, 18, 3, 0, 0, 0, ...   \n",
       "1  [2, 3501, 2503, 3534, 110, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "2  [2, 37, 1761, 107, 3536, 143, 107, 1801, 17, 8785, 17, 1908, 3, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3  [2, 192, 2801, 5047, 1411, 201, 389, 1525, 1850, 314, 480, 16107, 159, 219, 885, 1831,...   \n",
       "4  [2, 1963, 11, 83, 7565, 179, 155, 2801, 2096, 11, 83, 10315, 15768, 8244, 2157, 10340,...   \n",
       "\n",
       "                                                                              token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                                                              attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "1  [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...   \n",
       "\n",
       "                                                                                    word_ids  \\\n",
       "0  [None, 0, 1, 1, 2, 3, 4, 5, 5, 6, 7, 7, 7, 8, None, None, None, None, None, None, None...   \n",
       "1  [None, 0, 1, 1, 1, None, None, None, None, None, None, None, None, None, None, None, N...   \n",
       "2  [None, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None...   \n",
       "3  [None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 7, 7, 8, 9, 10, 10, 11, 12, 13, 14, 14, 15, 16, 1...   \n",
       "4  [None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 8, 9, 10, 10, 10, 11, 11, 11, 11, 12, 13, 14, ...   \n",
       "\n",
       "                                                                                      labels  \\\n",
       "0  [-100, 3, 0, -100, 7, 0, 0, 0, -100, 7, 0, -100, -100, 0, -100, -100, -100, -100, -100...   \n",
       "1  [-100, 1, 2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -...   \n",
       "2  [-100, 5, -100, -100, -100, -100, -100, 0, -100, -100, -100, -100, -100, -100, -100, -...   \n",
       "3  [-100, 0, 3, 4, 0, 0, 0, -100, -100, 0, 0, -100, -100, 0, 7, 0, -100, 0, 0, 0, 0, -100...   \n",
       "4  [-100, 5, 0, -100, 0, 0, 0, 3, 4, 0, -100, 0, -100, 0, 1, -100, -100, 2, -100, -100, -...   \n",
       "\n",
       "                                                                                       words  \\\n",
       "0                                 [EU, rejects, German, call, to, boycott, British, lamb, .]   \n",
       "1                                                                         [Peter, Blackburn]   \n",
       "2                                                                     [BRUSSELS, 1996-08-22]   \n",
       "3  [The, European, Commission, said, on, Thursday, it, disagreed, with, German, advice, t...   \n",
       "4  [Germany, 's, representative, to, the, European, Union, 's, veterinary, committee, Wer...   \n",
       "\n",
       "                                                                                        text  \n",
       "0                                            EU rejects German call to boycott British lamb.  \n",
       "1                                                                            Peter Blackburn  \n",
       "2                                                                    BRUSSELS 1996 - 08 - 22  \n",
       "3  The European Commission said on Thursday it disagreed with German advice to consumers ...  \n",
       "4  Germany's representative to the European Union's veterinary committee Werner Zwingmann...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>word_ids</th>\n",
       "      <th>labels</th>\n",
       "      <th>words</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2, 14613, 15718, 144, 15734, 17, 47, 145, 15718, 4679, 5626, 12414, 106, 137, 139, 14...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 6, 6, 7, 7...</td>\n",
       "      <td>[-100, 0, -100, -100, -100, 0, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100...</td>\n",
       "      <td>[CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, TOP, AFTER, INNINGS, VICTORY, .]</td>\n",
       "      <td>CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2, 47, 116, 140, 146, 116, 140, 1801, 17, 8785, 17, 1450, 3, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[None, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None...</td>\n",
       "      <td>[-100, 5, -100, -100, -100, -100, -100, 0, -100, -100, -100, -100, -100, -100, -100, -...</td>\n",
       "      <td>[LONDON, 1996-08-30]</td>\n",
       "      <td>LONDON 1996 - 08 - 30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2, 1730, 2703, 457, 17, 2945, 156, 2712, 4365, 6593, 1589, 902, 200, 3623, 201, 7767,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "      <td>[None, 0, 1, 2, 2, 2, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 12, 12, 13, 14, 14, 1...</td>\n",
       "      <td>[-100, 7, 8, 0, -100, -100, -100, 1, 2, -100, 0, 0, 0, 0, 0, 0, 0, 3, -100, -100, -100...</td>\n",
       "      <td>[West, Indian, all-rounder, Phil, Simmons, took, four, for, 38, on, Friday, as, Leices...</td>\n",
       "      <td>West Indian all - rounder Phil Simmons took four for 38 on Friday as Leicestershire be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2, 3551, 3556, 201, 1765, 16, 1743, 16, 829, 228, 1422, 17, 3802, 212, 2006, 7200, 70...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "      <td>[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, 10, 11, 12, 13, 14, 15, 15, 16, 17, 18, 19,...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, 3, 0, 3, -100, 0, 3, 0, 0, 0...</td>\n",
       "      <td>[Their, stay, on, top, ,, though, ,, may, be, short-lived, as, title, rivals, Essex, ,...</td>\n",
       "      <td>Their stay on top, though, may be short - lived as title rivals Essex, Derbyshire and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2, 946, 18672, 1913, 3394, 189, 489, 200, 7801, 201, 155, 3449, 4645, 246, 13264, 211...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...</td>\n",
       "      <td>[None, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 14, 14, 14, 15, 16, 17,...</td>\n",
       "      <td>[-100, 0, 0, 3, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 3, -100, -100, -100, 0, 0...</td>\n",
       "      <td>[After, bowling, Somerset, out, for, 83, on, the, opening, morning, at, Grace, Road, ,...</td>\n",
       "      <td>After bowling Somerset out for 83 on the opening morning at Grace Road, Leicestershire...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                   input_ids  \\\n",
       "0  [2, 14613, 15718, 144, 15734, 17, 47, 145, 15718, 4679, 5626, 12414, 106, 137, 139, 14...   \n",
       "1  [2, 47, 116, 140, 146, 116, 140, 1801, 17, 8785, 17, 1450, 3, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [2, 1730, 2703, 457, 17, 2945, 156, 2712, 4365, 6593, 1589, 902, 200, 3623, 201, 7767,...   \n",
       "3  [2, 3551, 3556, 201, 1765, 16, 1743, 16, 829, 228, 1422, 17, 3802, 212, 2006, 7200, 70...   \n",
       "4  [2, 946, 18672, 1913, 3394, 189, 489, 200, 7801, 201, 155, 3449, 4645, 246, 13264, 211...   \n",
       "\n",
       "                                                                              token_type_ids  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "\n",
       "                                                                              attention_mask  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...   \n",
       "\n",
       "                                                                                    word_ids  \\\n",
       "0  [None, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 4, 5, 6, 6, 7, 7...   \n",
       "1  [None, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, None, None, None, None, None, None, None, None...   \n",
       "2  [None, 0, 1, 2, 2, 2, 2, 3, 4, 4, 5, 6, 7, 8, 9, 10, 11, 12, 12, 12, 12, 13, 14, 14, 1...   \n",
       "3  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 9, 9, 10, 11, 12, 13, 14, 15, 15, 16, 17, 18, 19,...   \n",
       "4  [None, 0, 1, 2, 2, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 14, 14, 14, 15, 16, 17,...   \n",
       "\n",
       "                                                                                      labels  \\\n",
       "0  [-100, 0, -100, -100, -100, 0, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100...   \n",
       "1  [-100, 5, -100, -100, -100, -100, -100, 0, -100, -100, -100, -100, -100, -100, -100, -...   \n",
       "2  [-100, 7, 8, 0, -100, -100, -100, 1, 2, -100, 0, 0, 0, 0, 0, 0, 0, 3, -100, -100, -100...   \n",
       "3  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, -100, 0, 0, 0, 3, 0, 3, -100, 0, 3, 0, 0, 0...   \n",
       "4  [-100, 0, 0, 3, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 0, 3, -100, -100, -100, 0, 0...   \n",
       "\n",
       "                                                                                       words  \\\n",
       "0              [CRICKET, -, LEICESTERSHIRE, TAKE, OVER, AT, TOP, AFTER, INNINGS, VICTORY, .]   \n",
       "1                                                                       [LONDON, 1996-08-30]   \n",
       "2  [West, Indian, all-rounder, Phil, Simmons, took, four, for, 38, on, Friday, as, Leices...   \n",
       "3  [Their, stay, on, top, ,, though, ,, may, be, short-lived, as, title, rivals, Essex, ,...   \n",
       "4  [After, bowling, Somerset, out, for, 83, on, the, opening, morning, at, Grace, Road, ,...   \n",
       "\n",
       "                                                                                        text  \n",
       "0                           CRICKET - LEICESTERSHIRE TAKE OVER AT TOP AFTER INNINGS VICTORY.  \n",
       "1                                                                      LONDON 1996 - 08 - 30  \n",
       "2  West Indian all - rounder Phil Simmons took four for 38 on Friday as Leicestershire be...  \n",
       "3  Their stay on top, though, may be short - lived as title rivals Essex, Derbyshire and ...  \n",
       "4  After bowling Somerset out for 83 on the opening morning at Grace Road, Leicestershire...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained(tokenizer_dir)\n",
    "\n",
    "spc_tok_attr = {\"word_ids\": [None], \"labels\": [null_label]}\n",
    "cls_token = {**tokenizer(tokenizer.cls_token, add_special_tokens=False), **spc_tok_attr}\n",
    "sep_token = {**tokenizer(tokenizer.sep_token, add_special_tokens=False), **spc_tok_attr}\n",
    "pad_token = {**tokenizer(tokenizer.pad_token, add_special_tokens=False), **spc_tok_attr, \"attention_mask\": [0]}\n",
    "\n",
    "def process_dataset(ds, null_label=null_label, max_length=max_length, add_special_tokens=True, num_proc=4):\n",
    "    def process_sample(sample):\n",
    "        encoding = tokenizer(sample[\"tokens\"], add_special_tokens=False)\n",
    "        \n",
    "        # propagate word ids (based on words from sample[\"tokens\"])\n",
    "        encoding[\"word_ids\"] = [[i] * len(input_ids) for i, input_ids in enumerate(encoding[\"input_ids\"])]  \n",
    "\n",
    "        # propagate ner tags as labels\n",
    "        encoding[\"labels\"] = [[tag] + [null_label] * (len(input_ids)-1) \n",
    "                              for tag, input_ids in zip(sample[\"ner_tags\"], encoding[\"input_ids\"])]\n",
    "\n",
    "        # concat\n",
    "        encoding = {k: sum(v, []) for k, v in encoding.items()}\n",
    "        expected_encoding_length = len(encoding[\"input_ids\"]) + (2 if add_special_tokens else 0)\n",
    "        if max_length is not None:\n",
    "            pad_length = max_length - expected_encoding_length\n",
    "\n",
    "        for k, v in encoding.items():\n",
    "\n",
    "            # append info from special_tokens\n",
    "            if add_special_tokens:\n",
    "                v = cls_token[k] + v + sep_token[k]\n",
    "\n",
    "            # sanity check 1\n",
    "            assert len(v) == expected_encoding_length, f\"expected {k} of length {expected_encoding_length}, got {len(v)}\"\n",
    "\n",
    "            # padding / truncation\n",
    "            if max_length is not None:\n",
    "                v = v + pad_token[k] * pad_length if pad_length>0 else v[:max_length]\n",
    "\n",
    "                # sanity check 2\n",
    "                assert len(v) == max_length\n",
    "\n",
    "            encoding[k] = v\n",
    "\n",
    "        # provide concatenated text and a copy of the words\n",
    "        encoding[\"words\"] = sample[\"tokens\"]\n",
    "        encoding[\"text\"] = tokenizer.decode(encoding[\"input_ids\"], skip_special_tokens=True)\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "    return ds.map(process_sample, remove_columns=ds.features, num_proc=num_proc)\n",
    "\n",
    "train_dataset = process_dataset(dataset[\"train\"])\n",
    "val_dataset = process_dataset(dataset[\"validation\"])\n",
    "test_dataset = process_dataset(dataset[\"test\"])\n",
    "\n",
    "display(pd.DataFrame(train_dataset[:5]))\n",
    "display(pd.DataFrame(val_dataset[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For evaluation\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "eval_dataset = val_dataset\n",
    "\n",
    "def get_entities(pred, sample):\n",
    "    valid_sequence_length = sum(sample[\"attention_mask\"]) - 2   # subtract cls / sep tokens\n",
    "    pred = torch.argmax(pred, axis=-1).flatten()[1:valid_sequence_length+1]\n",
    "    pred_idxs = torch.nonzero(pred).flatten()\n",
    "\n",
    "    words = {i: word for i, word in enumerate(sample[\"words\"])}\n",
    "    word_ids = sample[\"word_ids\"][1:valid_sequence_length+1]\n",
    "\n",
    "    entities = OrderedDict()\n",
    "    for idx in pred_idxs:\n",
    "        word = words.get(word_ids[idx], \"[INV]\")\n",
    "        entities[word] = entities.get(word, []) + [int(pred[idx])]\n",
    "\n",
    "    return entities\n",
    "\n",
    "def eval_random_samps(eval_preds, n_samps=5):\n",
    "    preds = eval_preds.predictions\n",
    "\n",
    "    print(\"\\nEVALUATING ON RANDOM SAMPLES:\\n\")\n",
    "    for idx in np.random.permutation(len(preds))[:n_samps]:\n",
    "        sample = eval_dataset[int(idx)]\n",
    "        pp.pprint(sample[\"text\"])\n",
    "        pp.pprint(get_entities(torch.from_numpy(preds[idx]), sample))\n",
    "        print()\n",
    "\n",
    "    return dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../_data/pretrain/model were not used when initializing BertForTokenClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ../_data/pretrain/model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForTokenClassification, Trainer, TrainingArguments, TrainerCallback\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(path.join(data_dir, \"pretrain\", \"model\"), num_labels=num_classes)\n",
    "\n",
    "train_args = TrainingArguments(output_dir = ner_dir,\n",
    "                                  overwrite_output_dir = True,\n",
    "                                  **training_args)\n",
    "\n",
    "trainer = Trainer(model = model,\n",
    "                  args = train_args,\n",
    "                  compute_metrics=eval_random_samps,\n",
    "                  train_dataset = train_dataset,\n",
    "                  eval_dataset = eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: text, words, word_ids. If text, words, word_ids are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 14041\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 220\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cfa9440642b485c9109426f7a4c98d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set don't have a corresponding argument in `BertForTokenClassification.forward` and have been ignored: text, words, word_ids. If text, words, word_ids are not expected by `BertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 3250\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4908, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53d05cd6f00b44a8a61b5781a1671e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/407 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../_data/ner\n",
      "Configuration saved in ../_data/ner/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATING ON RANDOM SAMPLES:\n",
      "'World Group II, first round ( March 1 - 2 )'\n",
      "OrderedDict([('World', [3]), ('Group', [5]), ('II', [4])])\n",
      "\n",
      "'M. Maynard run out 1'\n",
      "OrderedDict([('M.', [3]), ('Maynard', [2])])\n",
      "\n",
      "'Manchester City 3 1 0 2 2 3 3'\n",
      "OrderedDict([('Manchester', [3, 4]), ('City', [4])])\n",
      "\n",
      "'NFL AMERICAN FOOTBALL - RANDALL CUNNINGHAM RETIRES.'\n",
      "OrderedDict()\n",
      "\n",
      "(\"The detention of veteran dissident Wang Donghai showed China's determination to crush any vestige of dissent during \"\n",
      " \"the current profound transitions in the nation's leadership, a human rights activist said on Saturday.\")\n",
      "OrderedDict([('Donghai', [1, 5]), ('China', [5])])\n",
      "\n",
      "{'eval_loss': 0.32919201254844666, 'eval_runtime': 102.5562, 'eval_samples_per_second': 31.69, 'eval_steps_per_second': 3.969, 'epoch': 1.0}\n",
      "{'train_runtime': 1531.6525, 'train_samples_per_second': 9.167, 'train_steps_per_second': 0.144, 'train_loss': 0.4907617742365057, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../_data/ner/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_model(ner_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "samples = np.random.permutation(len(val_dataset))[:5]\n",
    "samples = [val_dataset[int(i)] for i in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def to_tensor(sample):\n",
    "    return torch.tensor(sample).view(1,-1).to(device)\n",
    "\n",
    "preds = [model(input_ids=to_tensor(sample[\"input_ids\"]),\n",
    "               attention_mask=to_tensor(sample[\"attention_mask\"]),\n",
    "               token_type_ids=to_tensor(sample[\"token_type_ids\"])\n",
    "               ).logits.cpu() for sample in samples]\n",
    "\n",
    "pred_ents = [get_entities(pred, sample) for pred, sample in zip(preds, samples)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Derbyshire, nine - wicket winners over Worcestershire, and Surrey, who thrashed Warwickshire by an innings and 164 '\n",
      " 'runs, can instead take the day off along with rivals Leicestershire, who beat Somerset inside two days.')\n",
      "OrderedDict([('winners', [1]), ('Surrey', [3]), ('thrashed', [7]), ('Leicestershire', [3])])\n",
      "\n",
      "'Fulham 4 3 0 1 5 3 9'\n",
      "OrderedDict([('Fulham', [3, 4])])\n",
      "\n",
      "'Mahala is a Moslem village on Bosnian Serb republic territory.'\n",
      "OrderedDict([('Moslem', [7]), ('Bosnian', [7]), ('Serb', [7])])\n",
      "\n",
      "('Nyerere arrived in Rome this week on a private visit and held talks with the U. S. special envoy to Burundi, Howard '\n",
      " \"Wolpe, and the Sant'Egidio Community, an Italian Roman Catholic organisation which has been monitoring Burundi \"\n",
      " 'closely.')\n",
      "OrderedDict([ ('Rome', [7]),\n",
      "              ('U.S.', [5, 1]),\n",
      "              ('envoy', [1]),\n",
      "              ('Burundi', [5, 7]),\n",
      "              ('Howard', [1]),\n",
      "              ('Egidio', [3]),\n",
      "              ('Italian', [5]),\n",
      "              ('Roman', [1])])\n",
      "\n",
      "('A boisterous cheering section backed the distracted Chilean and booed the lanky American, who ate up all the '\n",
      " 'attention.')\n",
      "OrderedDict([('boisterous', [5]), ('Chilean', [5]), ('American', [7])])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample, pred in zip(samples, pred_ents):\n",
    "    pp.pprint(sample[\"text\"])\n",
    "    pp.pprint(pred)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pii')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c6a09ae61f2c394f67ff51dc946b62f1df5f97b79879dc2b09d91cc9b837b3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
