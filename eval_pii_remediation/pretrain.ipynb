{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from typing import Optional\n",
    "\n",
    "__DIR__ = globals()['_dh'][0]\n",
    "data_dir = path.relpath(path.join(__DIR__, \"..\", \"_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Settings\n",
    "_colab_install = True\n",
    "_pm_log_sections = False\n",
    "_testing = True\n",
    "\n",
    "# Parameters\n",
    "dataset = path.join(data_dir, \"wiki\", \"20220301.en.1gb\")\n",
    "\n",
    "base_model = \"bert-base-cased\"\n",
    "max_length = 128\n",
    "vocab_size = 20_000\n",
    "\n",
    "tokenize_params = dict(batched=True, num_proc=4)\n",
    "tokenizer_dir = path.join(data_dir, \"pretrain\", \"tokenizer\")\n",
    "\n",
    "mlm_probability = 0.15\n",
    "bert_config = dict()\n",
    "training_args = dict(\n",
    "    optim = \"adamw_torch\",\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 64,\n",
    "    eval_accumulation_steps = 10,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    logging_steps = 5000,\n",
    "    save_steps = 5000,\n",
    "    save_total_limit = 5,\n",
    ")\n",
    "max_eval_samples: Optional[int] = 2000\n",
    "model_dir = path.join(data_dir, \"pretrain\", \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if _testing:\n",
    "    dataset = path.join(data_dir, \"wiki\", \"20220301.en.test\")\n",
    "\n",
    "    training_args.update(dict(\n",
    "        max_steps = 3,\n",
    "        logging_steps = 1,\n",
    "    ))\n",
    "\n",
    "    max_eval_samples = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process settings / parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TESTING Parameters:\n",
      "OrderedDict([ ('dataset', '../_data/wiki/20220301.en.test'),\n",
      "              ('base_model', 'bert-base-cased'),\n",
      "              ('max_length', 128),\n",
      "              ('vocab_size', 20000),\n",
      "              ('tokenize_params', {'batched': True, 'num_proc': 4}),\n",
      "              ('tokenizer_dir', '../_data/pretrain/tokenizer'),\n",
      "              ('mlm_probability', 0.15),\n",
      "              ('bert_config', {}),\n",
      "              ( 'training_args',\n",
      "                { 'eval_accumulation_steps': 10,\n",
      "                  'evaluation_strategy': 'steps',\n",
      "                  'logging_steps': 1,\n",
      "                  'max_steps': 3,\n",
      "                  'num_train_epochs': 3,\n",
      "                  'optim': 'adamw_torch',\n",
      "                  'per_device_train_batch_size': 64,\n",
      "                  'save_steps': 5000,\n",
      "                  'save_total_limit': 5}),\n",
      "              ('max_eval_samples', 1000),\n",
      "              ('model_dir', '../_data/pretrain/model')])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "from collections import OrderedDict\n",
    "\n",
    "if _colab_install:\n",
    "    try:\n",
    "        import google.colab\n",
    "        \n",
    "        colab_install_script = path.join(__DIR__, \"..\", \"colab_install.sh\")\n",
    "\n",
    "        if not path.isfile(colab_install_script):\n",
    "            script_url = \"https://raw.githubusercontent.com/yenson-lau/pii-remediation/main/colab_install.sh\"\n",
    "            !wget $script_url -O $colab_install_script\n",
    "\n",
    "        !bash $colab_install_script\n",
    "\n",
    "    except ModuleNotFoundError:\n",
    "        pass\n",
    "\n",
    "if _pm_log_sections:\n",
    "    def pm_log_section(message):\n",
    "        print(f\"\\n[===== {message} =====]\\n\")\n",
    "else:\n",
    "    def pm_log_section(message):\n",
    "        return\n",
    "\n",
    "if _testing:\n",
    "    pm_log_section(\"Running on testing mode\")\n",
    "\n",
    "config = OrderedDict(\n",
    "    dataset = dataset,\n",
    "\n",
    "    base_model = base_model,\n",
    "    max_length = max_length,\n",
    "    vocab_size = vocab_size,\n",
    "\n",
    "    tokenize_params = tokenize_params,\n",
    "    tokenizer_dir = tokenizer_dir,\n",
    "\n",
    "    mlm_probability = mlm_probability,\n",
    "    bert_config = bert_config,\n",
    "    training_args = training_args,\n",
    "    max_eval_samples = max_eval_samples,\n",
    "    model_dir = model_dir,\n",
    ")\n",
    "\n",
    "print(f\"{'TESTING ' if _testing else ''}Parameters:\")\n",
    "pprint(config, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e5574e295c9a56a3\n",
      "Reusing dataset json (/Users/yenson/.cache/huggingface/datasets/json/default-e5574e295c9a56a3/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d2a79e7a0d49ec84003d25cc43b602",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-e7bfbf373754ac09\n",
      "Reusing dataset json (/Users/yenson/.cache/huggingface/datasets/json/default-e7bfbf373754ac09/0.0.0/a3e658c4731e59120d44081ac10bf85dc7e1388126b92338344ce9661907f253)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d88e00f32214556a8ff09bf37057b04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "pm_log_section(\"Loading dataset\")\n",
    "\n",
    "ds_dir = dataset\n",
    "dataset = dict()\n",
    "for split in [\"train\", \"val\"]:\n",
    "    data_file = path.join(ds_dir, f\"{split}_data.json\")\n",
    "    if not path.isfile(data_file):  data_file += \".gz\"\n",
    "    dataset[split] = load_dataset(\"json\", data_files=data_file, field=\"data\")[\"train\"]\n",
    "\n",
    "    if ((split != \"train\") \n",
    "        and (max_eval_samples is not None) \n",
    "        and (len(dataset[split]) > max_eval_samples)):\n",
    "\n",
    "        dataset[split] = dataset[split].select(range(max_eval_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "pm_log_section(\"Tokenizing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = (BertTokenizerFast\n",
    "                .from_pretrained(base_model)\n",
    "                .train_new_from_iterator(dataset[\"train\"][\"text\"], vocab_size))\n",
    "tokenizer.model_max_length = max_length\n",
    "\n",
    "tokenizer.save_pretrained(tokenizer_dir);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85334e5a6c1445f5be5064390b84eb52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e6203d820f44f399284bf20769b00e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ede803a446b46228228ce4993d2a078",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d424f6ae2ea44431a537c83c344cad63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1534f2f4bc4558b0574559acdff85b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35fbf12d6bc147a586a347899b9d36e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9645586ca48b433db0c3dd6ec7b7a21b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f720fe4378974e5683cdb6c11003d6ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenize_function = lambda ex: tokenizer(ex[\"text\"], truncation=True)\n",
    "\n",
    "tokenized_dataset = {\n",
    "    k: v.map(tokenize_function, remove_columns = list(v.features), **tokenize_params)\n",
    "    for k, v in dataset.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train masked language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import (BertConfig,\n",
    "                          BertForMaskedLM,\n",
    "                          DataCollatorForLanguageModeling,\n",
    "                          Trainer,\n",
    "                          TrainingArguments)\n",
    "\n",
    "pm_log_section(\"Training MLM\")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer = tokenizer,\n",
    "                                                mlm_probability = mlm_probability)\n",
    "\n",
    "bert_config = BertConfig(vocab_size = tokenizer.vocab_size, **bert_config)\n",
    "model = BertForMaskedLM(config = bert_config)\n",
    "\n",
    "training_args = TrainingArguments(output_dir = model_dir,\n",
    "                                  overwrite_output_dir = True,\n",
    "                                  **training_args)\n",
    "\n",
    "# def compute_metrics(eval_preds):\n",
    "#     idxs0, idxs1 = np.where(eval_preds.label_ids!=-100)\n",
    "\n",
    "#     preds = np.argmax(eval_preds.predictions[idxs0, idxs1, :], axis=-1)\n",
    "#     labels = eval_preds.label_ids[idxs0, idxs1]\n",
    "\n",
    "#     acc = (preds==labels).sum()/len(preds)\n",
    "\n",
    "#     return {\"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n",
      "***** Running training *****\n",
      "  Num examples = 8788\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 64\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cae13b16d7b4c51b7edca6a5a8e3fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 998\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 10.1149, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5167c0bce26a41b49c922ae4547d3570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.627870559692383, 'eval_runtime': 20.0141, 'eval_samples_per_second': 49.865, 'eval_steps_per_second': 6.246, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 998\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.6646, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.01}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e07c4b0a7e4cc69776bfaf56216f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.453207015991211, 'eval_runtime': 20.6477, 'eval_samples_per_second': 48.335, 'eval_steps_per_second': 6.054, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 998\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 9.5463, 'learning_rate': 0.0, 'epoch': 0.02}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1fdf08b4bc248268bde3ecb9bd75be7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../_data/pretrain/model\n",
      "Configuration saved in ../_data/pretrain/model/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 9.416359901428223, 'eval_runtime': 20.1873, 'eval_samples_per_second': 49.437, 'eval_steps_per_second': 6.192, 'epoch': 0.02}\n",
      "{'train_runtime': 75.0642, 'train_samples_per_second': 2.558, 'train_steps_per_second': 0.04, 'train_loss': 9.775258700052897, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../_data/pretrain/model/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model = model,\n",
    "                  args = training_args,\n",
    "                  data_collator = data_collator,\n",
    "                  train_dataset = tokenized_dataset[\"train\"],\n",
    "                  eval_dataset=tokenized_dataset[\"val\"])\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm_log_section(\"Finished pretraining!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 998\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f607e5eb890a4abe91483f4fad2a633f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 9.39639663696289,\n",
       " 'eval_runtime': 20.0376,\n",
       " 'eval_samples_per_second': 49.806,\n",
       " 'eval_steps_per_second': 6.238,\n",
       " 'epoch': 0.02}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()  # 9.39675235748291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('pii')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c6a09ae61f2c394f67ff51dc946b62f1df5f97b79879dc2b09d91cc9b837b3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
