data_dir: "{__dir__}/../_data"

dataset:
  wiki:
    hf_config: 20220301.en
    split_sizes_MB: {train: 1024, val: 128, test: 128}
    shuffle: true
    random_seed: 0
    sent_min_spaces: 5
    sent_max_spaces: 200
    output_subdir: "wiki/20220301.en.1gb"
    save_compressed: true

    .test_override:
      split_sizes_MB: {train: 1, val: .1, test: .1}
      output_subdir: "wiki/20220301.en.test"
      save_compressed: false

pretrain:
  dataset: wiki
  remediate_pii: false

  vocab_size: 20_000
  base_model: bert-base-cased
  max_length: 128
  tokenizer_subdir: "_pretrain/tokenizer"
  model_subdir: "_pretrain/model"

  tokenize_params: {}

  mlm_probability: 0.15

  bert_config: {}

  training_args:
    optim: adamw_torch
    num_train_epochs: 3
    per_device_train_batch_size: 128
    save_total_limit: 2
    save_steps: 500

    .test_override:
      max_steps: 3
      logging_steps: 1
      evaluation_strategy: steps
      # save_steps: 1

finetune: null

evaluate: null
